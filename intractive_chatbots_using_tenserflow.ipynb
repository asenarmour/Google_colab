{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "intractive chatbots using tenserflow.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPyCNY6uvZvuHrEwd6jnfFJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asenarmour/Google_colab/blob/master/intractive_chatbots_using_tenserflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpHT0cuOKZGs",
        "colab_type": "text"
      },
      "source": [
        "#Intractive Chatbots Using Tenserflow:\n",
        "this is based on the course which i took on [youtube](https://www.youtube.com/playlist?list=PLC0PzjY99Q_XdCbrn55FuG79I2z2vmAda) and it includes the the topics that were thought on it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8e12jn1nLhjp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36cl6prULCHx",
        "colab_type": "text"
      },
      "source": [
        "### Tokenization\n",
        "it is a task to segment string into words\n",
        "\n",
        "Using spacy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BS5FQt0vMxNl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import en_core_web_sm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G15ZlV-YKMWW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp=en_core_web_sm.load()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Yyzquy6LtMp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "7ec4e44b-8e1c-4cc2-edba-d8305f70ec92"
      },
      "source": [
        "example=nlp(\"this example is of text tokenization\")\n",
        "for token in example:\n",
        "  print(token.text)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "this\n",
            "example\n",
            "is\n",
            "of\n",
            "text\n",
            "tokenization\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FV9LIMhVNsq0",
        "colab_type": "text"
      },
      "source": [
        "### stemming\n",
        "it alter words to its stem form\n",
        ">\n",
        ">running-run\n",
        ">etc\n",
        "\n",
        "it has drawbacks like :\n",
        ">\n",
        ">this-thi\n",
        ">\n",
        ">here s is eliminated from the end which should'nt be the case"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAu8rPu-Novv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.stem import PorterStemmer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4asKdYXBOYqV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "85b25b5d-14dc-44fe-95f6-174d8e624abd"
      },
      "source": [
        "stemmer=PorterStemmer()\n",
        "example2=\"cat running was\"\n",
        "example2=[stemmer.stem(token) for token in example2.split(\" \")]\n",
        "print(\" \".join(example2))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cat run wa\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgR97-6-PaJN",
        "colab_type": "text"
      },
      "source": [
        "### Lemmatization\n",
        "\n",
        "unlike stemming it finds the root of words not just stem\n",
        "\n",
        "animals-animal\n",
        "\n",
        "(is,am,are)-be"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKS_NyL3O4Sn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0e76f7d2-2c08-4e28-ecf7-d3208053278b"
      },
      "source": [
        "example3=nlp(\"animals\")\n",
        "for token in example3:\n",
        "  print(token.lemma_)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "animal\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ca67udZjP_WU",
        "colab_type": "text"
      },
      "source": [
        "### Word Emedding\n",
        "\n",
        "these vectors contains words relation\n",
        "\n",
        "Vectorization:\n",
        "\n",
        "it is a process of turning a document in a numerical vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDW_-D9XT_-y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import en_core_web_sm\n",
        "nlp=en_core_web_sm.load()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjnJEAtFULgR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "example4=nlp(\"man woman king queen\")\n",
        "for token1 in example4:\n",
        "  for token2 in example4:\n",
        "    print(token1.text,token2.text,token1.similarity(token2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIF8hbN4WJkg",
        "colab_type": "text"
      },
      "source": [
        "### Named Entity Recognition\n",
        "\n",
        "will use en_core_web_sm from spacy which is already defined in the above cell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1k6RvMHUp0-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "38d30251-7919-48ec-c21b-285c5515e7aa"
      },
      "source": [
        "example5=\"Google is a company founded by Larry Page and Sergey Brin in United States Of America\"\n",
        "doc=nlp(example5)\n",
        "for ent in doc.ents:\n",
        "  print(ent.text,ent.label_)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Google ORG\n",
            "Larry Page PERSON\n",
            "Sergey Brin PERSON\n",
            "United States Of America GPE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qm9lRo7pX2y3",
        "colab_type": "text"
      },
      "source": [
        "### Sentiment Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PK1k0_yY57U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "f334139e-fbce-4ab3-8290-162f4b3cd42b"
      },
      "source": [
        "!pip install vaderSentiment"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting vaderSentiment\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/86/9e/c53e1fc61aac5ee490a6ac5e21b1ac04e55a7c2aba647bb8411c9aadf24e/vaderSentiment-3.2.1-py2.py3-none-any.whl (125kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 17.0MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 20kB 2.2MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 30kB 3.2MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 40kB 2.1MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 51kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 61kB 3.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 71kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 81kB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 92kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 102kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 112kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 122kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 3.5MB/s \n",
            "\u001b[?25hInstalling collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYbZXqxeW5Sz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-Rr-It-YI2f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "bdbe2156-3b9a-4599-b4f4-422e0be2c195"
      },
      "source": [
        "analyzer=SentimentIntensityAnalyzer()\n",
        "sentences=[\n",
        "           \"i love you\",\n",
        "           \"i hate you\",\n",
        "           \"i do not want to do this\",\n",
        "           \"i am going home\"\n",
        "]\n",
        "for sentence in sentences:\n",
        "  print(analyzer.polarity_scores(sentence)['compound'])"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6369\n",
            "-0.5719\n",
            "-0.0572\n",
            "0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_D68HSPFZhIG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}